{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Final Project: Sales Prediction"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/competitive-data-science-predict-future-sales/items.csv\n/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv\n/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv\n/kaggle/input/competitive-data-science-predict-future-sales/test.csv\n/kaggle/input/competitive-data-science-predict-future-sales/shops.csv\n/kaggle/input/competitive-data-science-predict-future-sales/sample_submission.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import sklearn\nimport scipy.sparse \nfrom itertools import product\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nfrom tqdm.notebook import tqdm as tqdm_notebook\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline \n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for p in [np, pd, sklearn, scipy, sns]:\n    print (p.__name__, p.__version__)","execution_count":3,"outputs":[{"output_type":"stream","text":"numpy 1.18.1\npandas 1.0.3\nsklearn 0.22.2.post1\nscipy 1.4.1\nxgboost 1.0.2\nseaborn 0.10.0\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/sales_train.csv')\nshops = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/shops.csv')\nitems = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/items.csv')\nitem_cats = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/item_categories.csv')\ntest = pd.read_csv('/kaggle/input/competitive-data-science-predict-future-sales/test.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Define functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast_dtypes(df):\n    \n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(*args):\n    \n    \"\"\" Funcion that calculates the root mean squared error\"\"\"\n    return np.sqrt(mean_squared_error(*args))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clip20(x):\n    return np.clip(x, 0, 20)\n\ndef clip40(x):\n    return np.clip(x, 0, 20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_feature_matrix(sales, test, items, list_lags, date_block_threshold):\n  \n    # Create \"grid\" with columns\n    index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n    # For every month we create a grid from all shops/items combinations from that month\n    grid = [] \n    new_items = pd.DataFrame()\n    cur_items_aux=np.array([])\n    for block_num in sales['date_block_num'].unique():\n        cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n        cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].append(pd.Series(cur_items_aux)).unique()\n        cur_items_aux = cur_items[pd.Series(cur_items).isin(test.item_id)]\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n    # Turn the grid into a dataframe\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n    # Add submission shop_id-item_id in order to test predictions\n    test['date_block_num'] = 34\n    grid = grid.append(test[['shop_id', 'item_id', 'date_block_num']])\n\n    # Groupby data to get shop-item-month aggregates\n    gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'})\n    gb = gb.rename(columns={'item_cnt_day': 'target'})\n    \n    # Join it to the grid\n    all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)\n\n    # Same as above but with shop-month aggregates\n    \n    gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n    gb = gb.rename(columns={'item_cnt_day': 'target_shop'})\n    all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n    # Same as above but with item-month aggregates\n    \n    gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':'sum'})\n    gb = gb.rename(columns={'item_cnt_day':'target_item'})\n    all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\n    # Downcast dtypes from 64 to 32 bit to save memory\n    all_data = downcast_dtypes(all_data)\n    del grid, gb \n    gc.collect()\n    # List of columns that we will use to create lags\n    cols_to_rename = list(all_data.columns.difference(index_cols)) \n\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n\n    # Don't use old data from year 2013\n    all_data = all_data[all_data['date_block_num'] >= date_block_threshold] \n\n    # List of all lagged features\n    fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n    # We will drop these at fitting stage\n    to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \n\n    # Category for each item\n    item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()\n\n    all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')\n    all_data = downcast_dtypes(all_data)\n    gc.collect();\n    \n    return [all_data, to_drop_cols]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## EDA"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1.4)\nsales_month = pd.DataFrame(sales.groupby(['date_block_num']).sum().item_cnt_day).reset_index()\nsales_month.columns = ['date_block_num', 'total_sales']\nsns.barplot(x ='date_block_num', y='total_sales', \n            data=sales_month.reset_index());\nplt.plot(sales_month.total_sales)\nplt.title('Total Sales per Month')\ndel sales_month","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1)\nsales_month_shop_id = pd.DataFrame(sales.groupby(['shop_id']).sum().item_cnt_day).reset_index()\nsales_month_shop_id.columns = ['shop_id', 'total_sales']\nsns.barplot(x ='shop_id', y='total_sales', data=sales_month_shop_id, palette='Paired')\nplt.title('Total Sales per Shop');\ndel sales_month_shop_id","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_context(\"talk\", font_scale=1.4)\nsales_item_id = pd.DataFrame(sales.groupby(['item_id']).sum().item_cnt_day)\nplt.xlabel('Item id')\nplt.ylabel('Sales')\nplt.plot(sales_item_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(1, 2)\naxes[0].title.set_text('Total Number of Items sold per day')\naxes[1].title.set_text('Total Price of Items sold per day')\nsns.boxplot( data=sales['item_cnt_day'],  orient='v' , ax=axes[0])\nsns.boxplot( data=sales['item_price'],  orient='v' , ax=axes[1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Leakage\nTotal number of Shop_id Item_id combinations in the test set. So we only have to concentrate on these combinations in our model training."},{"metadata":{"trusted":true},"cell_type":"code","source":"tuples_df = pd.Series(list(sales[['item_id', 'shop_id']].itertuples(index=False, name=None)))\ntuples_test = pd.Series(list(test[['item_id', 'shop_id']].itertuples(index=False, name=None)))\nprint(str(round(tuples_df.isin(tuples_test).sum()/len(tuples_df),2)*100)+'%')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prepare Dataset\nWe can prepare feature matrix with shops and items combinations. Also add future data (date_block_num=34) for predictions. We will also add lags to our data which will help our model to make predictions from history."},{"metadata":{"trusted":true},"cell_type":"code","source":"list_lags = [1, 2, 3, 4]\ndate_block_threshold = 6\nsales_for_modelling = sales[sales.item_id.isin(test.item_id)]\n[all_data, to_drop_cols]  = get_feature_matrix(sales_for_modelling, test, items, list_lags, date_block_threshold)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all_data = downcast_dtypes(all_data)\nall_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train-Validation-Test Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Keep test data separate\nsub_data = all_data[all_data.date_block_num==34].fillna(0)\nall_data = all_data[all_data.date_block_num<34].fillna(0)\nsub_data.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Keeping months 30-33 data for validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"dates = all_data['date_block_num']\nboolean_test = (dates.isin([30,31,32,33])) # & (boolean)\nboolean_train = ~boolean_test\ndates_train = dates[boolean_train]\ndates_val  = dates[boolean_test]\n\nX_train = all_data.loc[boolean_train].drop(to_drop_cols, axis=1)\nX_val =  all_data.loc[boolean_test].drop(to_drop_cols, axis=1)\n\ny_train = all_data.loc[boolean_train, 'target'].values\ny_val =  all_data.loc[boolean_test, 'target'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('X_train shape is ' + str(X_train.shape))\nprint('X_val shape is ' + str(X_val.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'Cross-validation is the {round(X_val.shape[0]/X_train.shape[0],2)*100} %' )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuples_validation_submission = pd.Series(list(X_val[['item_id', 'shop_id']][dates_val==33].itertuples(index=False, name=None)))\nprint(f'The {round(tuples_test.isin(tuples_validation_submission).sum()/len(tuples_test),2)*100} % of the item_id-shop_id are in the cv set ')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Random Forest Model\n### Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"X = X_train.append(X_val)\ny = np.concatenate([y_train, y_val])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=12,\n           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=300, n_jobs=4, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf.fit(X, clip40(y))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_rf_val = clip20(rf.predict(X_val.fillna(0)))\nprint('Train RMSE for rf is %f' % rmse(clip20(y_train), clip20(rf.predict(X_train))))\nprint('Val RMSE for rf is %f' % rmse(clip20(y_val), pred_rf_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_importances = pd.Series(rf.feature_importances_, index=X_val.columns)\nfeat_importances = feat_importances.nlargest(20)\nfeat_importances.plot(kind='barh')\nplt.title('Feature importance RF')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Prediction and Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_pred = rf.predict(sub_data.drop(to_drop_cols, axis=1).fillna(0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    os.unlink('submission.csv')\nexcept OSError:\n    pass","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = pd.DataFrame()\npredictions['shop_id'] = test.shop_id\npredictions['item_id'] = test.item_id\npredictions['item_cnt_month'] = test_pred\nsubmision = test[['ID', 'shop_id', 'item_id']].merge(predictions, on=['shop_id', 'item_id'], how='left').fillna(0)\nsubmision[['ID', 'item_cnt_month']].to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}